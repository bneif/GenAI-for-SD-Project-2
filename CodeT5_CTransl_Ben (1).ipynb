{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch==2.5.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (908.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.20.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.20.1%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.5.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Installing collected packages: triton, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0+cu124\n",
      "    Uninstalling torchaudio-2.6.0+cu124:\n",
      "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.5.1+cu124 torchaudio-2.5.1+cu124 torchvision-0.20.1+cu124 triton-3.1.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting tree_sitter==0.2.0\n",
      "  Downloading tree_sitter-0.2.0.tar.gz (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.4/110.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: tree_sitter\n",
      "  Building wheel for tree_sitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tree_sitter: filename=tree_sitter-0.2.0-cp311-cp311-linux_x86_64.whl size=389533 sha256=e1dd64cfd23edea8297e968c6e2c58dfddb7453dafe8291df6f4dcfaeb680685\n",
      "  Stored in directory: /root/.cache/pip/wheels/d9/6e/e2/b0126ad4f531cf09749b69518118f0ebf7bf3134ed91c71abb\n",
      "Successfully built tree_sitter\n",
      "Installing collected packages: tree_sitter\n",
      "Successfully installed tree_sitter-0.2.0\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face - Fine-Tuning CodeT5 for Code Translation (AI4SE Focus)\n",
    "\n",
    "# This notebook demonstrates how to fine-tune the CodeT5 model using Hugging Face Transformers\n",
    "# for a Software Engineering task: translating Python code to Java.\n",
    "\n",
    "# ------------------------\n",
    "# 1. Install Required Libraries\n",
    "# ------------------------\n",
    "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install transformers datasets evaluate -q\n",
    "! pip install transformers\n",
    "!pip install tree_sitter==0.2.0\n",
    "! git clone -q https://github.com/microsoft/CodeXGLUE.git\n",
    "!pip install evaluate\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 2. Load Dataset (CodeXGLUE - Code Translation Java <=> C#)\n",
    "# ------------------------------------------------------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# CodeXGLUE is a benchmark dataset collection by Microsoft for code-related tasks.\n",
    "\n",
    "# Here, we use the provided training, validation, and testing datasets.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# training_data = pd.read_csv(filepath_or_buffer=\"/content/ft_train.csv\", dtype={'cleaned_method':str, 'target_block':str, 'tokens_in_method':int})\n",
    "# testing_data = pd.read_csv(filepath_or_buffer=\"/content/ft_test.csv\", dtype={'cleaned_method':str, 'target_block':str, 'tokens_in_method':int})\n",
    "# validation_data = pd.read_csv(filepath_or_buffer=\"/content/ft_valid.csv\", dtype={'cleaned_method':str, 'target_block':str, 'tokens_in_method':int})\n",
    "\n",
    "\n",
    "# Calculate the number of rows to read for each dataset (initially used to try this on a smaller dataset)\n",
    "training_rows = int(1 * 50000)  # % of 50000\n",
    "testing_rows = int(1 * 5000)   # % of 5000\n",
    "validation_rows = int(1 * 5000) # % of 5000\n",
    "\n",
    "# Read the first 10% of each dataset - using 1% of the rows is temporary while I fine-tune the model\n",
    "training_data = pd.read_csv(filepath_or_buffer=\"/content/ft_train.csv\", dtype={'cleaned_method':str, 'target_block':str, 'tokens_in_method':int}, nrows=training_rows)\n",
    "testing_data = pd.read_csv(filepath_or_buffer=\"/content/ft_test.csv\", dtype={'cleaned_method':str, 'target_block':str, 'tokens_in_method':int}, nrows=testing_rows)\n",
    "validation_data = pd.read_csv(filepath_or_buffer=\"/content/ft_valid.csv\", dtype={'cleaned_method':str, 'target_block':str, 'tokens_in_method':int}, nrows=validation_rows)\n",
    "\n",
    "\n",
    "# print(validation_data.head())\n",
    "# validation_data.loc[0][0]\n",
    "# print(\"test\")\n",
    "# print(flatten_and_tabize_string(validation_data.iloc[0][0]))\n",
    "\n",
    "# OLD CODE FROM THE PROVIDED VERSION OF THIS FILE BELOW:\n",
    "\n",
    "# # Here, we use the code-translation-python-java dataset.\n",
    "# dataset = load_dataset(\"google/code_x_glue_cc_code_to_code_trans\")\n",
    "\n",
    "# # Dataset contains: 'train', 'validation', 'test' splits\n",
    "# print(\"Sample Python Code:\", dataset['train'][0]['java'])\n",
    "# print(\"Target Java Code:\", dataset['train'][0]['cs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFAYpCpZH-Vq"
   },
   "source": [
    "✅ This following loads a pre-trained models & tokenizer from Hugging Face using the checkpoint name (e.g., \"Salesforce/codet5-small\").\n",
    "\n",
    "\n",
    "*  The tokenizer knows how to convert text into tokens that the model\n",
    "\n",
    "*   It also handles things like padding, truncation, special tokens, etc.\n",
    "\n",
    "*\tIt comes with a fixed vocabulary learned during pretraining, that however we can expand if needed as shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32102, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 3. Load Pre-trained Model & Tokenizer\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# 3. Load Pre-trained Model & Tokenizer\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Check device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_checkpoint = \"Salesforce/codet5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Move model to the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# Add special tokens if needed\n",
    "tokenizer.add_tokens([\"<IF-STMT>\", \"<TAB>\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from transformers import T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "# from transformers import RobertaTokenizer\n",
    "# from datasets import DatasetDict\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "# from transformers import EarlyStoppingCallback\n",
    "\n",
    "# model_checkpoint = \"Salesforce/codet5-small\"\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "# tokenizer.add_tokens([\"<IF-STMT>\", \"<TAB>\"]) #Imagine we need an extra token. This line adds the extra token to the vocabulary\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzM8CS0uJUOi"
   },
   "source": [
    "⚠️⚠️⚠️ If you add new tokens like this, you must also resize the model’s embedding layer: model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "Otherwise, the model won’t know what to do with the new token IDs!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx1t-qM7HybE"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6b33b3cf014c01b90e23509e2a5194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc766e9ad84430bb8ed03de8323a9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1818ce272edc44d1bab58b49f6f9d642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "# 4. We prepare now the fine-tuning dataset using the tokenizer we preloaded\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def remove_spaces(my_str):\n",
    "  return my_str.replace(\" \", \"\")\n",
    "\n",
    "def compare_tokens(a, b): # Checks if two tokens (strings) are the same, up to starting with the Ġ character\n",
    "  if len(a) == len(b):\n",
    "    for i in range(len(a)):\n",
    "      if a[i] != b[i]:\n",
    "        return False\n",
    "    #print(a+\" and \"+b)\n",
    "    return True\n",
    "  if len(a) > len(b) and a[0] == \"Ġ\":\n",
    "    return compare_tokens(a[1:], b)\n",
    "  if len(b) > len(a) and b[0] == \"Ġ\":\n",
    "    return compare_tokens(b[1:], a)\n",
    "  return False\n",
    "\n",
    "# 4. Prepare the fine-tuning dataset using the tokenizer\n",
    "def flatten_mask_tabize(my_str, target_block):\n",
    "    my_str = my_str.replace(\"\\n\", \" \")  # Flatten\n",
    "    my_str = my_str.replace(\"    \", \" <TAB>\")  # Tabize\n",
    "    tokenized_str = tokenizer.tokenize(my_str)  # Tokenize\n",
    "    tokenized_target = tokenizer.tokenize(target_block)\n",
    "\n",
    "    # Mask If Statement\n",
    "    for i in range(len(tokenized_str) - len(tokenized_target)):\n",
    "        for j in range(len(tokenized_target)):\n",
    "            if not compare_tokens(tokenized_str[i + j], tokenized_target[j]):\n",
    "                break\n",
    "            if j == len(tokenized_target) - 1:\n",
    "                tokenized_str = tokenized_str[:i] + [\"<IF-STMT>\"] + tokenized_str[i + len(tokenized_target):]\n",
    "                return tokenized_str\n",
    "    return tokenized_str\n",
    "\n",
    "def preprocess_dataset(my_dataframe):\n",
    "    my_dataframe['processed_method'] = ''\n",
    "    for i in range(len(my_dataframe)):\n",
    "        my_dataframe.loc[i, 'processed_method'] = tokenizer.convert_tokens_to_string(flatten_mask_tabize(my_dataframe.iloc[i].iloc[0], my_dataframe.iloc[i].iloc[1]))\n",
    "    return my_dataframe\n",
    "\n",
    "# Convert datasets to Hugging Face Datasets and preprocess\n",
    "training_data = preprocess_dataset(training_data)\n",
    "validation_data = preprocess_dataset(validation_data)\n",
    "testing_data = preprocess_dataset(testing_data)\n",
    "\n",
    "# Convert processed methods to lists of strings\n",
    "training_data['processed_method'] = training_data['processed_method'].apply(lambda x: [x])\n",
    "validation_data['processed_method'] = validation_data['processed_method'].apply(lambda x: [x])\n",
    "testing_data['processed_method'] = testing_data['processed_method'].apply(lambda x: [x])\n",
    "\n",
    "# Convert Pandas DataFrames to Hugging Face Datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "training_data = Dataset.from_pandas(training_data)\n",
    "validation_data = Dataset.from_pandas(validation_data)\n",
    "testing_data = Dataset.from_pandas(testing_data)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset[\"train\"] = training_data\n",
    "dataset[\"validation\"] = validation_data\n",
    "dataset[\"test\"] = testing_data\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"processed_method\"]\n",
    "    targets = examples[\"target_block\"]\n",
    "\n",
    "    # Flatten the list of lists to a single list of strings\n",
    "    inputs = [item for sublist in inputs for item in sublist]\n",
    "\n",
    "    # Convert inputs and targets to token IDs before padding\n",
    "    inputs = tokenizer(inputs, truncation=True, padding=False)[\"input_ids\"]\n",
    "    labels = tokenizer(targets, truncation=True, padding=False)[\"input_ids\"]\n",
    "\n",
    "    # Pad sequences manually\n",
    "    max_length = 1000\n",
    "    inputs = [x + [tokenizer.pad_token_id] * (max_length - len(x)) for x in inputs]\n",
    "    labels = [x + [tokenizer.pad_token_id] * (max_length - len(x)) for x in labels]\n",
    "\n",
    "    return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "# Apply the preprocess function\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=['cleaned_method', 'target_block', 'tokens_in_method', 'processed_method'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-f6ec8d8599e0>:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33332' max='33332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33332/33332 2:25:50, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.002931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.002869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=33332, training_loss=0.008082446082621781, metrics={'train_runtime': 8751.0092, 'train_samples_per_second': 22.855, 'train_steps_per_second': 3.809, 'total_flos': 5.286260441088e+16, 'train_loss': 0.008082446082621781, 'epoch': 3.9996})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Define Training Arguments and Trainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import Adafactor\n",
    "\n",
    "from transformers import Adafactor\n",
    "\n",
    "# optimizer = Adafactor(\n",
    "#     model.parameters(),\n",
    "#     lr=5e-5,                # manually set learning rate\n",
    "#     relative_step=False,    # must disable relative_step\n",
    "#     warmup_init=False       # must disable warmup_init if not using relative_step\n",
    "# )\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./codet5-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=3,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100, # initially 100\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    #optimizers=(optimizer, None),  # Pass the optimizer here\n",
    ")\n",
    "\n",
    "# ------------------------\n",
    "# 6. Train the Model\n",
    "# ------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation Metrics: {'eval_loss': 0.002970718080177903, 'eval_runtime': 69.3469, 'eval_samples_per_second': 72.101, 'eval_steps_per_second': 36.051, 'epoch': 3.9996}\n",
      "F1 Score: 0.7407\n",
      "BLEU Score Calculation Output: ngram match: 0.7253817006576274, weighted ngram match: 0.729027673186805, syntax_match: 0.7073335130763009, dataflow_match: 0.7406679764243614\n",
      "CodeBLEU score:  0.7256027158362737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [28:58<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Input Function with Masked If Condition  \\\n",
      "0  def read(self, count=True, timeout=None, ignor...   \n",
      "1  def _cache_mem(curr_out, prev_mem, mem_len, re...   \n",
      "2  def filtered(gen): <TAB> for example in gen: <...   \n",
      "3  def search(self, query): <TAB> # \"Search.ashx?...   \n",
      "4  def _check_script(self, script, directive): <T...   \n",
      "5  def getAllDataLinkIDs(): <TAB> linkDataIDs = s...   \n",
      "6  def _stderr_supports_color(): <TAB> try: <TAB>...   \n",
      "7  def offsets(self): <TAB> offsets = {} <TAB> of...   \n",
      "8  def Restore(self): <TAB> picker, obj = self._w...   \n",
      "9  def dt_s_tup_to_string(dt_s_tup): <TAB> dt_str...   \n",
      "\n",
      "   Exact Match (true/false)  \\\n",
      "0                     False   \n",
      "1                      True   \n",
      "2                      True   \n",
      "3                      True   \n",
      "4                     False   \n",
      "5                      True   \n",
      "6                      True   \n",
      "7                     False   \n",
      "8                     False   \n",
      "9                     False   \n",
      "\n",
      "                               Expected If Condition  \\\n",
      "0          if ignore_timeouts and is_timeout ( e ) :   \n",
      "1                              if prev_mem is None :   \n",
      "2                      if example_len > max_length :   \n",
      "3         if item . get ( \"type\" , \"\" ) == \"audio\" :   \n",
      "4                    if var . must_contain ( \"/\" ) :   \n",
      "5                                if socketID [ 1 ] :   \n",
      "6            if curses . tigetnum ( \"colors\" ) > 0 :   \n",
      "7                    if offset_so_far % align != 0 :   \n",
      "8                        if type ( value ) == list :   \n",
      "9  if \"co\" in dt_string or \"ci\" in dt_string or \"...   \n",
      "\n",
      "                       Predicted If Condition  CodeBLEU Score (0-100)  \\\n",
      "0   if ignore_non_errors and is_noerr ( e ) :                    0.44   \n",
      "1                       if prev_mem is None :                    0.75   \n",
      "2               if example_len > max_length :                    0.75   \n",
      "3  if item . get ( \"type\" , \"\" ) == \"audio\" :                    0.75   \n",
      "4              if var . can_contain ( \".\" ) :                    0.34   \n",
      "5                         if socketID [ 1 ] :                    0.75   \n",
      "6     if curses . tigetnum ( \"colors\" ) > 0 :                    0.75   \n",
      "7                                  if align :                    0.02   \n",
      "8                      if len ( value ) > 1 :                    0.18   \n",
      "9                  if len ( dt_string ) > 2 :                    0.01   \n",
      "\n",
      "   BLEU-4 Score (0-100)  \n",
      "0                   0.0  \n",
      "1                   0.0  \n",
      "2                   0.0  \n",
      "3                   0.0  \n",
      "4                   0.0  \n",
      "5                   0.0  \n",
      "6                   0.0  \n",
      "7                   0.0  \n",
      "8                   0.0  \n",
      "9                   0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_dbd606bd-2c3a-4d8b-b33c-4f569c1ae52b\", \"testset-results.csv\", 3028453)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_cfb501ca-3773-460a-9a8d-0ea7d795a68d\", \"predictions.txt\", 163770)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_017b846c-ceea-45bc-851b-1eda6f27c054\", \"targets.txt\", 169492)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_e8f04c85-1fdc-418e-8d7d-ce69a8bd7bd6\", \"bleuresults.txt\", 180)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 7. Evaluate on Test Set\n",
    "# ------------------------\n",
    "metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test Evaluation Metrics:\", metrics)\n",
    "\n",
    "# ------------------------\n",
    "# 8. Test Code Translation\n",
    "# ------------------------\n",
    "\n",
    "# input_code = \"def add(a, b):\\n    return a + b\"\n",
    "# inputs = tokenizer(input_code, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to device\n",
    "\n",
    "# outputs = model.generate(**inputs, max_length=256)\n",
    "# print(\"Generated Java Code:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# 8. Generate Predictions and Save to Files\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for example in tokenized_datasets[\"test\"]:\n",
    "    input_code = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True)  # Decode from input_ids to get original masked code\n",
    "    target_code = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)   # Decode from labels to get target code\n",
    "\n",
    "    # Tokenize and generate predictions\n",
    "    inputs = tokenizer(input_code, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    output = model.generate(**inputs, max_length=256)\n",
    "\n",
    "    # Decode predictions and store them\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    predictions.append(decoded_output)\n",
    "    targets.append(target_code)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Convert to exact match labels: 1 for correct, 0 for incorrect\n",
    "exact_matches = [pred.strip() == tgt.strip() for pred, tgt in zip(predictions, targets)]\n",
    "y_true = [1] * len(exact_matches)  # Ground truth is always 1 (correct if it matches)\n",
    "y_pred = [1 if match else 0 for match in exact_matches]  # Predicted match or not\n",
    "\n",
    "# Compute F1 score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Save predictions and targets to text files\n",
    "with open(\"/content/predictions.txt\", \"w\") as pred_file:\n",
    "    for pred in predictions:\n",
    "        pred_file.write(pred + \"\\n\")\n",
    "\n",
    "with open(\"/content/targets.txt\", \"w\") as target_file:\n",
    "    for target in targets:\n",
    "        target_file.write(target + \"\\n\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "# Define the command for BLEU metric calculation\n",
    "bleu_command = [\n",
    "    \"python\", \"/content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/calc_code_bleu.py\",\n",
    "    \"--refs\", \"/content/targets.txt\",\n",
    "    \"--hyp\", \"/content/predictions.txt\",\n",
    "    \"--lang\", \"java\",\n",
    "    \"--params\", \"0.25,0.25,0.25,0.25\"\n",
    "]\n",
    "\n",
    "# Change to the correct directory before running the command\n",
    "import os\n",
    "os.chdir(\"/content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU\")\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    result = subprocess.run(bleu_command, check=True, capture_output=True, text=True)\n",
    "    with open(\"/content/bleuresults.txt\", \"w\") as target_file:\n",
    "      target_file.write(result.stdout + \"\\n\")\n",
    "    print(\"BLEU Score Calculation Output:\", result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error during BLEU score calculation:\", e)\n",
    "    print(\"Error Output:\", e.stderr)\n",
    "\n",
    "import csv\n",
    "import subprocess\n",
    "import os\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure you're in the right directory for CodeBLEU script\n",
    "codebleu_script = \"/content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/calc_code_bleu.py\"\n",
    "codebleu_dir = os.path.dirname(codebleu_script)\n",
    "os.chdir(codebleu_dir)\n",
    "\n",
    "results = []\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "csv_file = \"/content/testset-results.csv\"\n",
    "\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\n",
    "        \"Input Function with Masked If Condition\",\n",
    "        \"Exact Match (true/false)\",\n",
    "        \"Expected If Condition\",\n",
    "        \"Predicted If Condition\",\n",
    "        \"CodeBLEU Score (0-100)\",\n",
    "        \"BLEU-4 Score (0-100)\"\n",
    "    ])\n",
    "\n",
    "    for example in tqdm(tokenized_datasets[\"test\"]):\n",
    "        input_code = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True)\n",
    "        target_code = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "        inputs = tokenizer(input_code, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        output = model.generate(**inputs, max_length=256)\n",
    "        predicted_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append(predicted_code)\n",
    "        targets.append(target_code)\n",
    "\n",
    "        # Create temp files for this one example\n",
    "        with tempfile.NamedTemporaryFile(mode='w+', delete=False) as ref_file, \\\n",
    "             tempfile.NamedTemporaryFile(mode='w+', delete=False) as hyp_file:\n",
    "\n",
    "            ref_file.write(target_code.strip() + \"\\n\")\n",
    "            hyp_file.write(predicted_code.strip() + \"\\n\")\n",
    "            ref_file.flush()\n",
    "            hyp_file.flush()\n",
    "\n",
    "            # Run CodeBLEU\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\n",
    "                        \"python\", codebleu_script,\n",
    "                        \"--refs\", ref_file.name,\n",
    "                        \"--hyp\", hyp_file.name,\n",
    "                        \"--lang\", \"java\",\n",
    "                        \"--params\", \"0.25,0.25,0.25,0.25\"\n",
    "                    ],\n",
    "                    check=True, capture_output=True, text=True\n",
    "                )\n",
    "\n",
    "                codebleu_output = result.stdout.strip().splitlines()\n",
    "                # Parse scores from output safely\n",
    "                codebleu_score, bleu4_score = 0.0, 0.0\n",
    "                for line in codebleu_output:\n",
    "                    if \"CodeBLEU score:\" in line:\n",
    "                        codebleu_score = float(line.split(\":\")[-1].strip())\n",
    "                    elif \"BLEU-4:\" in line:\n",
    "                        bleu4_score = float(line.split(\":\")[-1].strip())\n",
    "\n",
    "                # with open(\"/content/bleuresults.txt\", \"w\") as target_file:\n",
    "                #     for line in codebleu_output:\n",
    "                #         if \"CodeBLEU score:\" in line:\n",
    "                #             codebleu_score = float(line.split(\":\")[-1].strip())\n",
    "                #             target_file.write(str(codebleu_score) + \"\\n\")\n",
    "                #         elif \"BLEU-4:\" in line:\n",
    "                #             bleu4_score = float(line.split(\":\")[-1].strip())\n",
    "                #             target_file.write(str(bleu4_score) + \"\\n\")\n",
    "\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(\"CodeBLEU error:\", e.stderr)\n",
    "                codebleu_score = 0.0\n",
    "                bleu4_score = 0.0\n",
    "\n",
    "        # Extract just the condition for display (you could refine this with regex if needed)\n",
    "        expected_condition = target_code.strip()\n",
    "        predicted_condition = predicted_code.strip()\n",
    "        exact_match = expected_condition == predicted_condition\n",
    "\n",
    "        writer.writerow([\n",
    "            input_code.strip(),\n",
    "            str(exact_match).lower(),\n",
    "            expected_condition,\n",
    "            predicted_condition,\n",
    "            round(codebleu_score, 2),\n",
    "            round(bleu4_score, 2)\n",
    "        ])\n",
    "\n",
    "\n",
    "# Load and print the first 10 rows\n",
    "df2 = pd.read_csv(\"/content/testset-results.csv\")\n",
    "print(df2.head(10))\n",
    "\n",
    "from google.colab import files\n",
    "files.download('/content/testset-results.csv')\n",
    "files.download('/content/predictions.txt')\n",
    "files.download('/content/targets.txt')\n",
    "files.download('/content/bleuresults.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
